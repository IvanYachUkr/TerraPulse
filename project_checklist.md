# ðŸ“‹ Mapping Urban Change in Nuremberg â€” Full Project Checklist (SOLO)

> **Course**: Machine Learning WT 25/26 â€” Grabocka, Asano, Frey â€” UTN  
> **Objective**: Build a tabular ML system that predicts land-cover composition and change in Nuremberg, deliver an interactive product with uncertainty communication.  
> **Mode**: **Solo** (ignore team assignment fields)

---

## Legend

| Symbol | Meaning |
|--------|---------|
| `[ ]`  | Not started |
| `[/]`  | In progress |
| `[x]`  | Done |
| âš ï¸     | Grading-critical / mandatory |
| ðŸ†     | Bonus / extra credit |

---

## Phase 0 â€” Project Setup & Reproducibility âš ï¸

- [x] Create GitHub repo + clean structure
- [x] `.gitignore` (geodata + large rasters excluded)
- [x] `requirements.txt` exists
- [ ] Reproducible install test on a clean env (fresh venv) âš ï¸
- [ ] Add a single â€œrun-allâ€ entrypoint (Makefile / run.sh) ðŸ†
- [ ] Ensure no raw data is in git history âš ï¸

---

## Phase 1 â€” Problem Framing & Scope Definition âš ï¸

- [x] Define land-cover classes (WorldCover â†’ 6 final classes)
- [x] Define spatial unit: **100m Ã— 100m**
- [x] Define temporal setup:
  - Labels: 2020 (v100), 2021 (v200)
  - Unlabeled: 2022â€“2025 (Sentinel-only, forward prediction for dashboard)
- [x] Define target task:
  - Multi-output regression: predict 6 proportions per cell (sumâ‰ˆ1)
  - Change Î” derived from predictions: Î” = p(T2) âˆ’ p(T1)
- [x] Define intended user + non-allowed decisions (limitations)
- [ ] Write **Scope Document** (`reports/scope.md`) âš ï¸
  - Must include the *algorithm version shift* caveat (v100 â†’ v200)

---

## Phase 2 â€” Data Acquisition & Preprocessing âš ï¸

### 2A â€” Sentinel-2 Seasonal Composites âš ï¸
- [x] Download seasonal composites for **6 labeled periods**:
  - 2020: spring, summer, autumn
  - 2021: spring, summer, autumn
- [x] All 6 composites verified:
  - Geometry assertion: CRS/transform/shape match canonical anchor
  - Sizes ~85â€“88 MB each
- [x] Known runtime issues fixed in pipeline:
  - `rescale=False` stackstac dtype/scale conflict
  - dtype / NaN fill-value compatibility
  - dask-aware finite checks

### 2B â€” WorldCover Labels âš ï¸
- [x] Download WorldCover 2020 v100 tile N48E009
- [x] Download WorldCover 2021 v200 tile N48E009
- [x] Reprojected to Sentinel anchor grid (nearest)
- [x] Class mapping (11 â†’ 6 classes)
- [x] **Important limitation** âš ï¸:
  - v100 vs v200 algorithm difference may create â€œfake changeâ€ signals

### 2C â€” InputQuality layer (optional)
- [x] Investigated InputQuality availability
- [x] **Decision: skip for now** (not public for our tile)
  - Use `valid_fraction` as the quality proxy
  - Can add InputQuality later without refactoring

### 2D â€” OSM Auxiliary Data ðŸ†
- [x] Downloaded OSM datasets (buildings/roads/landuse/natural/water)
- [x] Saved as GeoPackages in `data/raw/osm/`
- [ ] Decide final usage:
  - Option A: merge OSM once at final dataset stage (recommended)
  - Option B: suffix OSM per composite (creates lots of zero deltas)

---

## Phase 3 â€” Canonical Grid + Labels (Anchor-based) âš ï¸

- [x] Create canonical anchor raster (Phase 1 artifact)
- [x] Build grid from anchor blocks:
  - **100m cells = 10Ã—10 pixels**
  - **186 cols Ã— 161 rows = 29,946 cells**
  - `cell_id` row-major, contiguous 0..N-1
  - Output: `data/processed/v2/grid.gpkg`
  - **Grid size justification (retrospective analysis):**
    - Connected-component analysis of pixel-level change (2020â†’2021) in the AOI:
      76% of changed regions are â‰¤4 pixels, median = 2 px (200 mÂ²)
    - At 100m (100 px/cell), these tiny artifacts â†’ 1â€“4% proportion noise, naturally smoothed out
    - Real change (â‰¥50 px regions, ~1.5% of regions) still produces 15â€“25% proportion shifts per cell
    - Smaller cells (e.g. 50m = 25 px) would amplify label artifacts to ~16% per cell
    - Larger cells (e.g. 200m) would halve sample count to ~7,500 â€” too few for spatial CV
    - **100m is the sweet spot: noise-smoothing + sufficient samples + spatial granularity**
- [x] Aggregate label proportions per cell:
  - `data/processed/v2/labels_2020.parquet`
  - `data/processed/v2/labels_2021.parquet`
- [x] Compute change labels:
  - `data/processed/v2/labels_change.parquet` (delta = 2021âˆ’2020)

---

## Phase 4 â€” Feature Engineering (per composite) âš ï¸

### 4A â€” Core features (done) âš ï¸
- [x] Implement anchor-correct, join-safe, scale-safe extraction
  - Never drops `cell_id`
  - `cell_id â†’ (row_idx, col_idx)` computed deterministically
  - Reflectance scale auto-detected (likely /10000) for texture assumptions
  - Control flags:
    - `valid_fraction`, `low_valid_fraction`
    - `reflectance_scale`
    - `full_features_computed`
- [x] Core feature set per composite:
  - Band stats: mean/std/min/max/median/q25/q75/finite_frac Ã— 10 bands
  - Indices: NDVI/NDWI/NDBI/NDMI/NBR/SAVI/BSI/NDRE1/NDRE2 (stats)
  - Tasseled cap (brightness/greenness/wetness)
  - Spatial simple (edges, Laplacian, Moranâ€™s I, NDVI spread)
- [x] Outputs produced for all 6 composites:
  - 29,946 rows each
  - **143 non-cell columns** (139 real features + 4 control cols)
  - Low valid fraction rates: ~0.2%â€“1.7% depending on season

### 4B â€” Full feature set (done) âš ï¸
- [x] Full extraction completed (heavy features):
  - GLCM texture
  - Gabor
  - LBP histogram + entropy
  - HOG
  - Morphological profile features
  - Semivariogram features (+ fit params)
- [x] Full outputs verified:
  - All 6 composites: 29,946 cells Ã— 239 columns (238 features + cell_id)
  - Identical `cell_id` set across all composites
  - `full_features_computed` = 1 for non-low-VF cells
  - Scale detected as 10000 across all composites

### 4C â€” Known fixed bugs (documentable as â€œdata issuesâ€) âš ï¸
- [x] Bug 1: reflectance scaling mismatch (0..10000 vs 0..1 assumptions)
- [x] Bug 2: dropping cells based on quality threshold (join corruption risk)
- [x] Bug 3: cell_id order / mapping mismatch risk (row/col alignment)
- [x] Bug 4: imputation scope accidentally touching metadata columns

---

## Phase 5 â€” Merge + Delta Features (YOY + Seasonal Contrasts) âš ï¸

### 5A â€” Core merge/deltas (done) âš ï¸
- [x] Load all 6 composite feature tables
- [x] Build wide table with suffixes: `{feature}_{year}_{season}`
- [x] Compute deltas:
  - YoY: 2021âˆ’2020 per season â†’ `delta_yoy_{season}_{feat}`
  - Seasonal contrasts within each year â†’ `delta_{year}_{sB}_vs_{sA}_{feat}`
- [x] Output:
  - `data/processed/v2/features_merged_core.parquet`
  - **29,946 rows Ã— 2,110 columns**
  - 0% NaN after imputation (expected; indicators preserve quality info)

### 5B â€” Full merge/deltas (done) âš ï¸
- [x] Run compute_deltas.py on `feature-set=full`
- [x] Output:
  - `data/processed/v2/features_merged_full.parquet`
  - **29,946 rows Ã— 3,535 columns** (660 MB)
  - 1,429 base + 702 YoY + 1,404 seasonal deltas
  - 0% NaN after imputation
- [x] Defensive checks:
  - Year ordering sorted (prevent sign flip)
  - Feature set intersection across composites (warn on mismatch)
  - Delta symmetry spot-checked on random cells

---

## Phase 6 â€” EDA + Reality Check âš ï¸

> Must identify â‰¥3 non-trivial data issues. Must choose 1 you do NOT fix and justify why.

- [x] EDA script (`scripts/run_eda.py`) producing report-ready figures âš ï¸
  - Feature distributions (hist/violin/box) â€” Fig 1, 4a, 4b
  - Correlation heatmaps (top-20 Spearman) â€” Fig 5a
  - Label distribution + label-change distribution â€” Fig 1, 2
  - Spatial maps of key features + Î”labels â€” Fig 3a, 3b
  - Redundancy clustering + drift (YoY + seasonal) â€” Fig 5b, 5c, 5d
  - Quality coupling analysis (all VF cols, all features) â€” Fig 6
  - Reflectance scale verification â€” Fig 7
  - Moran's I spatial autocorrelation (vectorized, full grid) â€” Fig 10
  - Engineering validation checks â€” Fig 9
  - Feature manifest (CSV + Parquet) â€” 12 tables total
- [x] Data issues (minimum 3) âš ï¸
  - [x] Reflectance scale bug (fixed)
  - [x] Cell dropping / join corruption risk (fixed)
  - [x] Imputation scope (fixed)
  - [x] Choose one NOT fixed: **WorldCover v100â†’v200 label-version shift** âš ï¸
    - Justify: cannot correct without alternative ground truth; treat as label noise + discuss
- [x] Save all figures to `reports/phase6/core/figures/` âš ï¸
- [x] Hardened with 20+ defensive guardrails:
  - Row alignment + cross-table cell_id assertions
  - Label proportion invariants (sum-to-1, deltas sum-to-0)
  - CRS projection + grid geometry + row-major assumptions validated
  - NaN/Inf guards, numeric dtype filtering, empty-data guards
  - CLASS_COLORS/CLASS_NAMES/COMPOSITES consistency asserts

---

## Phase 7 â€” Train/Test Split Design (avoid spatial leakage) âš ï¸

- [x] Implement a spatially-aware split âš ï¸
  - Tile-based blocked split: 10Ã—10 cells (1 kmÂ²), 323 tile groups, 5-fold
  - Split indices saved to `data/processed/v2/split_spatial.parquet`
  - Metadata: `data/processed/v2/split_spatial_meta.json` (full repro params)
- [x] 6-way leakage comparison (Ridge regression) âš ï¸
  - Random: RÂ²=0.767 (baseline)
  - Grouped (scattered tiles): RÂ²=0.742 (gap +0.025)
  - Contiguous (row bands): RÂ²=0.691 (gap +0.076)
  - Morton Z-curve: RÂ²=0.527 (gap +0.240)
  - Region growing: RÂ²=0.429 (gap +0.338)
  - Region growing + buffer: RÂ²=0.386 (gap +0.381)
- [x] Morton Z-curve fold builder (bit-interleaved, explicit uint64)
- [x] Multi-start region growing fold builder (10 restarts, balance+contiguity scoring)
- [x] Chebyshev buffer exclusion zone (configurable buffer_tiles)
- [x] Buffer sweep figure (RÂ² vs separation distance)
- [x] Contiguity/balance/compactness metrics (`compute_fold_metrics`, BFS on tile graph)
  - Contiguous: connected=YES, max_dev=34.8%, compactness=1.878
  - Morton: connected=NO (fragments on 17Ã—19), max_dev=27.9%, compactness=1.765
  - Region growing: connected=YES, max_dev=2.1%, compactness=1.871
- [x] Exported tables: `leakage_comparison.csv`, `buffer_sweep.csv`, `fold_contiguity.csv`
- [x] Fold maps: grouped, contiguous, Morton, region growing
- [x] Unit tests: 14 invariant tests in `tests/test_splitting.py` (all pass in ~3s)
  - Partition correctness, tile integrity, connectedness, balance, determinism,
    buffer no-overlap, n_starts overflow, metrics schema
- [x] Hardening fixes applied:
  - Morton: explicit uint64 + bit extraction (no signed overflow risk)
  - Region growing: multi-start + scoring, n_starts guard, tile integrity assert
  - Leakage comparison: fold-size guard for degenerate folds
  - Buffer_m: `max(tile_h, tile_w)` for non-square tile safety
  - Compactness: isoperimetric scaling `boundary_edges / sqrt(n_tiles)`
  - Metadata: region growing params recorded for reproducibility

## Phase 8 â€” Modeling âš ï¸

> At least 2 models: one interpretable + one flexible.

- [x] Target: predict 2021 proportions from merged 2020+2021 seasonal features
- [x] Model 1 (interpretable) âš ï¸ â€” Ridge baseline
  - Ridge CV RÂ²â‰ˆ0.43 (region growing + buffer) â€” anchors "no leakage" claim
- [/] Model 2 (flexible) âš ï¸ â€” Trees + MLPs
- [/] Hyperparameter tuning (time-boxed) âš ï¸
- [ ] Compare models (performance vs interpretability)

### Model comparison (all evaluated with spatial CV + buffer)

| Model | RÂ² (uniform) | MAE (pp) | Notes |
|-------|-------------|----------|-------|
| Ridge | 0.527 | 5.21 | Interpretable baseline |
| ElasticNet | 0.434 | 5.43 | Sparse, but worse than Ridge |
| ExtraTrees | 0.675 | 3.34 | Best tree model (2109 feat) |
| RF | 0.684 | 3.23 | Competitive tree model |
| CatBoost | 0.671 | 4.00 | Gradient boosted, lag on MAE |
| **MLP (Phase 8 best)** | **0.752** | **2.92** | Plain MLP, single fold |

### Tree results (28 configs, complete â€” `trees_reduced_features.csv`)
- Best: **ExtraTrees RÂ²=0.692**, MAE=3.13pp (`all_core`, 2109 features)
- RF: RÂ²=0.684 | CatBoost: RÂ²=0.671 (but best Aitchison=4.19)
- Feature sets: `all_core` barely beats `bands_and_indices` (798 feats)
- Trees plateau at ~0.69 regardless of ensemble size or feature set

### MLP V2 sweep (superseded by V4)
- Best: **relu L5 h256 RÂ²=0.858**, MAE=2.55pp (`bands_indices`, 798 feats)
- MLPâ€“Tree gap: **+0.17 RÂ²** on fold-0 (0.86 vs 0.69), **+0.09 RÂ²** on CV means â€” MLPs learn cross-spectral interactions trees cannot
- âš ï¸ V2 had BatchNorm confound on ReLU â€” fixed in V4

### MLP V4 search sweep (complete â€” 1549/1584 configs, fold-0 only)
- Script: `scripts/run_mlp_overnight_v4.py --stage search` (fold-0, 120 epochs)
- Best overall: **`residual_gelu_L6_d512_bn` RÂ²=0.8634**, MAE=2.64pp
- Feature set ablation (best RÂ² per set):
  | Rank | Feature Set | Features | Best RÂ² |
  |------|---|---|---|
  | 1 | `bands_indices` | 798 | **0.8634** |
  | 2 | `bands_indices_glcm_lbp` | ~900 | 0.8598 |
  | 3 | `bands_indices_texture` | ~1100 | 0.8432 |
  | 4 | `full_no_deltas` | ~1400 | 0.8313 |
  | 5 | `bands_indices_hog` | ~850 | 0.8309 |
  | 6 | `all_full` | 3535 | 0.7952 |
  | 7 | `top500_full` | 500 | 0.6841 |
  | 8 | `texture_all` | ~300 | 0.6340 |
- Key finding: **more features = worse performance** â€” textures add noise at 100m resolution
- Architecture insights: residual + GELU + BatchNorm + lower LR (0.0005) wins
- Auto-CV launcher: `scripts/launch_cv_after_search.py` polls search â†’ selects
  top 20 global + top 10 per feature set â†’ launches CV (300 epochs Ã— 5 folds)

### MLP V4 CV (complete â€” 83 configs Ã— 5 folds = 415 runs)
- [x] CV complete
- Top 5 configs (5-fold mean Â± std):
  | Rank | Config | RÂ² mean | RÂ² std | MAE | Epochs |
  |------|--------|---------|--------|-----|--------|
  | 1 | `bands_indices_glcm_lbp_plain_mish_L5_d512_bn` | **0.775** | 0.074 | 2.49 | 300 (cap!) |
  | 2 | `bands_indices_residual_silu_L10_d256_nonorm` | **0.772** | 0.050 | 2.52 | 297 (cap!) |
  | 3 | `full_no_deltas_plain_gelu_L5_d512_bn` | **0.771** | 0.035 | 2.65 | 280 |
  | 4 | `bands_indices_plain_silu_L5_d512_bn` | **0.771** | 0.054 | 2.45 | 300 (cap!) |
  | 5 | `bands_indices_texture_plain_silu_L5_d256_bn` | 0.767 | 0.033 | 2.62 | 295 |
- Key observations:
  - Many top configs hit the **300-epoch cap** â€” potentially undertrained
  - `bands_indices` (798 feat) and `bands_indices_glcm_lbp` (924 feat) dominate
  - Plain architectures (mish, silu) + BN competitive with residual deep networks
  - Fold-0 RÂ²=0.86 drops to CV mean 0.77 â†’ **~0.09 fold variance** indicates spatial heterogeneity
- [/] Final model selection from CV results

### MLP V5 deep training (complete â€” 233 runs, 5.7h)
- Script: `scripts/run_mlp_v5_deep_train.py`
- Results: `reports/phase8/tables/mlp_v5_deep.csv`, `mlp_v5_deep_summary.csv`
- 233 runs across 5 folds, 3 feature sets, 2000-epoch cap (all early-stopped, none hit cap)
- **Best 5-fold CV models (RÂ² mean Â± std):**
  1. `glcm_lbp_plain_silu_L5_d1024_bn` â€” **RÂ²=0.787Â±0.041**, MAE=2.51pp
  2. `bands_indices_plain_mish_L5_d512_bn` â€” RÂ²=0.769Â±0.065, MAE=2.49pp
  3. `glcm_lbp_residual_gelu_L12_d256_bn` â€” RÂ²=0.768Â±0.036, MAE=2.57pp
  4. `bands_indices_residual_silu_L10_d256_nonorm` â€” RÂ²=0.768Â±0.054, MAE=2.51pp
  5. `full_no_deltas_plain_mish_L5_d512_bn` â€” RÂ²=0.762Â±0.040, MAE=2.67pp
- **Key findings:**
  - `bands_indices_glcm_lbp` (924 feat) beats `bands_indices` (798 feat) â€” GLCM+LBP texture helps
  - `full_no_deltas` (1428 feat) is worst â€” Gabor/HOG/morph/semivariogram add noise
  - Shallow plain (L5) dominates; deep residual (L12-16) competitive but not better
  - SiLU is the best activation; Mish close second; GeGLU catastrophically unstable
  - Texture features reduce fold-to-fold variance (Â±0.041 vs Â±0.065)
  - Fold RÂ² ranges: F0=0.86 (urban), F1=0.77 (mixed), F2=0.77 (forest), F3=0.73 (suburban), F4=0.83 (dense forest)
- [x] V5 results complete

### MLP V10: Definitive feature+architecture+head sweep (running)
- Script: `scripts/run_mlp_v10_definitive.py`
- 3 feature sets Ã— 3 architectures (SiLU) Ã— ILR + 6 GeGLU runs = 15 configs Ã— 5 folds = 75 runs
- Feature sets: bi_LBP (864f), bi_MP_Gabor (996f), bi_LBP_MP_Gabor (1060f)
- Key V10 findings (fold 0):
  - SiLU architectures dominate GeGLU, especially for Dirichlet head
  - bi_MP_Gabor L5 d1024 bn: strong all-rounder
  - Texture features (LBP, MP+Gabor) DO help at 10Ã—10 â€” previous "underperform" assessment was wrong

### MLP V11: New Gabor v2 + Morph DMP + Dirichlet (queued, auto-starts after V10)
- Script: `scripts/run_mlp_v11_new_texture.py`
- 7 configs Ã— 5 folds = 35 runs (~2h):
  | # | Features | Architecture | Head |
  |---|---|---|---|
  | 1 | bi_Gab2_DMP (~1566f) | L5 d1024 bn | ILR |
  | 2 | bi_Gab2_DMP | L5 d1024 bn | Dirichlet |
  | 3 | bi_LBP_Gab2_DMP (~1632f) | L5 d1024 bn | ILR |
  | 4 | bi_Gab2_DMP | L5 d1536 bn | ILR |
  | 5 | bi_Gab2_DMP | L7 d1024 bn | ILR |
  | 6 | bi_Gab2_DMP | L5 d1536 bn | Dirichlet |
  | 7 | bi_LBP_Gab2_DMP | L5 d1536 bn | ILR |
- New texture features: complex Gabor magnitude+phase (384f) + Morph DMP peak/valley (384f)
- Supplemental parquet: `features_texture_v2.parquet` (extracted via `extract_texture_v2.py`)

### Future: V6 deep+wide architecture sweep
- Test unexplored architecture region: L12-16 Ã— d512-1024
- V5 shows shallow wide (L5Ã—d1024) is the overall winner architecture
- Incorporate new spectral indices (EVI2, MNDWI, GNDVI, NDTI, IRECI, CRI1)
- Feature ablation study complete â€” RedEdge+VegIdx+TC (348 feat) achieves 98.6% of full bands+indices with half the features

### Future: V6/V7 advanced spectral indices
- Add to `spectral_indices()` in `extract_features.py`, then re-extract:
  - **MNDWI** = (Green âˆ’ SWIR1)/(Green + SWIR1) â€” better water in urban areas
  - **EVI2** = 2.5Ã—(NIR âˆ’ Red)/(NIR + 2.4Ã—Red + 1) â€” less saturated than NDVI
  - **NDTI** = (SWIR1 âˆ’ SWIR2)/(SWIR1 + SWIR2) â€” cropland vs bare soil
  - **IRECI** = (B07 âˆ’ B04)/(B05/B06) â€” Sentinel-2-specific red-edge chlorophyll
  - **GNDVI** = (NIR âˆ’ Green)/(NIR + Green) â€” chlorophyll-sensitive
  - **CRI1** = (1/Green) âˆ’ (1/RE1) â€” carotenoid content
- Note: NDMI â‰ˆ âˆ’NDBI (redundant), SAVI â‰ˆ NDVI â€” consider dropping duplicates
- Test new `bands_indices_v2` feature set vs current `bands_indices`

---

## Phase 9 â€” Evaluation Beyond Accuracy âš ï¸

- [ ] Standard metrics (per class)
- [ ] Change-specific metrics âš ï¸
  - false-change rate, stability in unchanged areas, Î”-magnitude calibration
- [ ] Stress tests (at least one) âš ï¸
  - noise injection, missing features, seasonal shift, spatial shift
- [ ] Failure analysis maps (where model is wrong + why) âš ï¸

---

## Phase 10 â€” Explainability + Uncertainty âš ï¸

- [ ] Feature importance + SHAP (for flexible model)
- [ ] One helpful explanation + one misleading explanation âš ï¸
- [/] Uncertainty proxy:
  - [x] Conformal prediction intervals computed (6 models Ã— 6 classes)
  - Ridge: 78â€“83% coverage, wide intervals (5â€“29pp)
  - MLP: 71â€“80% coverage, tighter intervals (0.03â€“14pp)
  - Tree models: 65â€“80% coverage, moderate intervals
  - âš ï¸ Coverage below nominal 90% â†’ conservative interpretation needed

---

## Phase 11 â€” Interactive Dashboard / Product âš ï¸

- [ ] Streamlit + Folium (recommended)
- [ ] Time selection, class selection, Î” visualization
- [ ] Uncertainty overlay + disclaimer panel âš ï¸
- [ ] Click cell â†’ show features + prediction + explanation ðŸ†

---

## Phase 12 â€” Technical Report âš ï¸ (â‰¤ 10 pages)

- [ ] Report skeleton with required sections
- [ ] Insert EDA figures + tables
- [ ] Explicitly discuss:
  - spatial leakage
  - label version shift (v100 vs v200)
  - whatâ€™s not validated beyond 2021

---

## Phase 13 â€” ChatGPT Reflection âš ï¸

- [ ] Log key prompts + outputs used
- [ ] â€œArguing Against ChatGPTâ€ case 1 + 2 (must be real, must have evidence)
- [ ] One misleading ChatGPT example (you already have good candidates from earlier bugs)

---

## Phase 14 â€” Demo Video âš ï¸ (â‰¤ 5 min)

- [ ] Script + record dashboard walkthrough
- [ ] Show: prediction, Î”, uncertainty, helpful vs misleading explanation, limitations

---

## Phase 15 â€” Repo Polish + Submission âš ï¸

- [ ] End-to-end reproducible run (fresh environment) âš ï¸
- [ ] Clean README: install, data fetch, pipeline steps, dashboard launch âš ï¸
- [ ] Final deliverables check (report PDF, code, product, video, ChatGPT log) âš ï¸

---

## Future: Rust Performance Rewrite ðŸ†

> For interactive "select any territory â†’ predict" product. Not needed for course submission.

- [ ] Rust feature extraction module (via PyO3 or standalone binary)
  - Raster I/O: `gdal` crate (C-speed reads, no Python overhead)
  - Numerical: `ndarray` + BLAS (NumPy-equivalent)
  - Parallelism: `rayon` for data-parallel cell processing across all CPU cores
  - Gabor/Morph/LBP: port ~500 lines of Python to compiled Rust
  - Expected speedup: 20-50Ã— over Python (CPU), 100Ã—+ with GPU path
- [ ] ONNX model export + Rust inference via `ort` crate
  - Export trained PyTorch model â†’ ONNX
  - Inference in Rust: no Python runtime needed at serving time
- [ ] Docker deployment
  - Multi-stage build: `rust:latest` â†’ `debian:slim` (~30 MB final image)
  - Or fully static musl binary â†’ `FROM scratch` (~10 MB image)
  - GDAL + PROJ packaged in build stage
- [ ] Alternative: PyTorch GPU batch extraction (simpler, stays in Python)
  - Process entire raster as one tensor, no per-cell loop
  - `F.conv2d` for Gabor, `F.max_pool2d` for morphology
  - Good enough for demo, ~2-5s per territory on GPU

